\documentclass{llncs}

\title{Versioned File Archiving, Compression, and Distribution}
\titlerunning{Versioned File Archiving and Compression}

\usepackage{dcc99}
\usepackage{enumerate}

\author{Josh MacDonald\inst{1}}

\authorrunning{Josh MacDonald}

\tocauthor{Josh MacDonald (University of California at Berkeley,
Department of Electrical Engineering and Computer Sciences.)  }

\institute{University of California at Berkeley, Department of
Electrical Engineering and Computer Sciences, Berkeley CA 94720, USA
}

\date{29 Oct 1998}

\nonfrenchspacing

%\theoremstyle{marginbreak}

\newtheorem{thrm}{{\sc Theorem}}
\newtheorem{clm}{{\sc Claim}}
\newtheorem{lemm}{{\sc Lemma}}
\newtheorem{alg}{{\sc Algorithm}}

%\Alph{alg}
\renewcommand{\thealg}{\Alph{alg}}

\begin{document}
\bibliographystyle{acm}

\maketitle
\begin{abstract}
The \xd {} system implements a technique for archiving and compressing
collections of many similar file versions.  It stores only the
differences between certain versions.  I describe and discuss an
algorithm for computing \emph{file deltas}, present measurements, and
demonstrate its application to versioned file-archival and efficient
file-distribution network protocols.
\end{abstract}

% Put up your benchmark, whatever it is, on a ftp site somewhere, and
% INCLUDE A MD5 CHECKSUM, traceability, etc.

\section{Overview}

The \emph{file delta} problem is to compute a small set of
instructions for transforming one file into another---one that is
expected to be a function of the file's changes, not its content.
This technique is well established for versioned file-archival.
Though the advantages of using file deltas to transmit changes over a
network are clear, specifying and widely deploying such a system
efficient enough to justify itself is not as easy as it seems.  There
are a number of issues to overcome.  First, the execution cost of
computing and compressing deltas can be prohibitive--a site
administrator might rather let everyone on the network suffer through
a congested network condition than be CPU-bound while delivering
efficient deltas to an uncongested network.  Second, efficiently a
integrating delta transmission into an application requires work; a
server must store and name multiple versions of a file, compute deltas
when requested, complicate existing protocols, and be fair---these are
not always compatible.

Recently there have been advances in delta algorithms, but their use
in network delta communication has been slow to follow.  This work
addresses many of these issues, and finishes with a few new challenges
that I am currently working on.  I have designed and implemented a
decentralized scheme for highly-available, versioned file-archival,
with which I have implemented a delta-based distribution and
replication protocol built upon a simple, distributed file-archive
abstraction.  The system, called \xd, is implemented as the basis of
multiple server, distributed version control in PRCS, the Project
Revision Control System \cite{MacDonald:1998:SCM}.

This paper outlines the \xd {} system in three sections: delta
algorithms (\S\ref{sec:delta}), file-archival (\S\ref{sec:archival}),
and distribution protocols (\S\ref{sec:distribution}).  Each section
will detail related work, discuss previous implementations, outline a
part of the system and measure it, and suggest future work.

\section{Delta Algorithms}\label{sec:delta}

The \emph{file delta problem} is to efficiently compute a delta $d$
that can be stored compactly and used to construct a \emph{To} file
$t^d$ from the set of $k^d$ \emph{From} files $F^d = \left\{ f^d_0
\ldots f^d_{k^d} \right\}$.  The \emph{generate} operation computes
$d$ from $F^d$ and $t^d$:

\begin{equation}
d = t^d \ominus F^d
\end{equation}

\noindent while the \emph{apply} operation computes the reverse:

\begin{equation}
t^d = d \oplus F^d
\end{equation}

For our purposes, a file will be defined to be a sequence of bytes.
For a file $f$, $\size(f)$ denotes the length of $f$ in bytes and set
of files $F$, $\size(F) = \sum_{f \in F} \size(f)$.  The notation,
$f[i]$, denotes the byte at offset $i$ of $f$, where $0 \le i <
size(f)$.

A good solution should be fast, perform well on large inputs, and
expect to benefit from preprocessing $F$ for repeated computations.
Performance considerations for real files and memory systems make it
desirable to operate in a single, sequential pass through $t$.

The \emph{copy/insert} class of delta algorithms are those that
produce a common delta encoding consisting of \emph{copy} and
\emph{insert} operations.  This class of algorithms is useful because
there is a straightforward definition of an optimal solution, and the
implementation of \emph{apply} is extremely simple.  For reasons to
come, this type of delta will be represented in two separate parts:
the \emph{control} $C^d$ and \emph{insert-data} $I^d$.  The control
contains a sequence of instructions $c^d_1 \ldots c^d_{z^d}$.  The
insert-data is represented as a file containing the concatenation of
the data inserted by each insert instruction.  Each instruction $c_i$
is one of the two operations:

\begin{align}
&\icpy{m}{o}{l} \\
&\iins{l}
\end{align}

\noindent These are executed in order to construct the \emph{To} file,
where \texttt{copy} inserts the substring $f_m[o] \ldots f_m[o+l-1]$,
and \texttt{insert} inserts the next $l$ bytes from $I^d$.

The \emph{insert/delete} class of algorithms is characterized by
\emph{insert} and \emph{delete} instructions.  Many existing tools for
computing file deltas use this type of encoding, as it produces a form
which is easy for humans to read.

Existing work on delta algorithms uses \emph{paired}
algorithms---those in which there is a single \emph{From} file.  For
reasons which will become clear, the definition used here is more
general than other presentations in allowing $F$ to be a set of files
rather than a single file.  Of course, $F$ can be reduced to a single
file by concatenation, but attempting to generalize paired algorithms
to multiple \emph{From} files violates many of the assumptions made to
improve these algorithms.

\subsection{Related Work}

Work on delta algorithms has been influenced by related work on the
longest common subsequence problem, data compression, and pattern
matching.

The first technique for computing file deltas used an insert/delete
algorithm based upon finding the longest common subsequence (LCS) of
two files.  Among these, the UNIX \texttt{diff} utility was first used
to list changes between two files in a line-by-line summary
\cite{Miller:1985:FCP}.  The \texttt{diff} utility is
\emph{line-oriented}---each insertion and deletion involves only
complete lines.  It was adopted by the early version control tools
SCCS \cite{Rochkind:ieee:tose:1975} and RCS \cite{Tichy:1985:RSV}.
Line-oriented algorithms, however, perform poorly on files which are
not line terminated such as images, rich-text data, and object files.

Tichy developed a linear time and space, greedy copy/insert algorithm
\cite{Tichy84} using a suffix tree to search for matching substrings
of the files in question.  This simple greedy approach will be
discussed next, and is easily shown to be optimal for copy/insert
deltas under certain assumptions.

Burns \cite{Burns:Thesis} presents a recent summary of previous work
and discusses several paired, copy/insert algorithms based upon a
greedy approach using hashed Rabin-Karp fingerprints for string
matching.  He then refines it in several ways, because the simplest
greedy algorithm requires space linear in the length of the file.  He
analyzes constant-space versions of this algorithm, making the
assumption that changes between the files are the result of only
insertions and deletions, for example.  When $F$ is a set of files,
however, many of these improvements fail.

Hunt, Tichy, and Vo present an impressive paired delta algorithm
called \emph{vdelta} \cite{Vo:1996:SCM}, which mixes Lempel-Ziv
compression and differential compression to compute a compressed delta
by using $F$ as a dictionary to compress $t$.  This particular
approach is difficult to compare with others, because most delta
algorithms compute uncompressed deltas.

A delta can be applied in-place if application can be performed using
the space containing the data to which the delta is applied.  An
\emph{insert/delete} delta can always be applied in-place, but an
arbitrary \emph{copy/insert} delta cannot.  Burns and Long have shown
techniques for computing \emph{copy/insert} deltas that can be applied
\emph{in-place} \cite{Burns:InPlace}.

\subsection{Difference Metrics} \label{sec:metrics}

Comparison of algorithms has been a difficult subject, as algorithms
are studied variously in terms of delta size, compressed delta size,
and optimality with a particular encoding.  A solution that is optimal
in one encoding cannot be easily compared with another encoding,
without an absolute metric.  Hunt, Tichy, and Vo suggest comparing
delta algorithms with a difference metric based upon the length of the
longest common subsequence \cite{Vo:1996:SCM}:

\begin{equation}
\difference = \frac{\size(f)+\size(t)}{2} - \size(\lcs(f,t))
\end{equation}

\noindent where $f$ is the single from input.  The $\difference$ value
is compared to the size of a delta produced by an algorithm, allowing
its performance to be judged relative to results of other algorithms.
This is troublesome for several reasons: there is no linear time and
space algorithm for computing the exact LCS; it does not apply to
multiple input files---since it does not allow permuted or repeated
copy regions; and it is not even the most appropriate dynamic
programming style of solution since it weighs an \emph{insert} the
same a \emph{delete} instruction.

The copy/insert encoding is not optimal either; but it allows for
permuted and repeated copied regions while the metric based on LCS
does not.  Additionally:

\begin{itemize}
\item An exact solution is known that requires $O(\size(F))$ space \cite{Tichy84}
\item It characterizes non-paired delta algorithms
\item The particular, uncompressed encoding is useful in practice
\end{itemize}

For these reasons, it seems appropriate to use the size of the
unencoded, optimal \emph{copy/insert} solution as a metric for
comparing all delta algorithms.  Also, using an encoding-independent
metric allows algorithm designers to study encoding separately from
search and pattern matching issues.

More precisely, the \emph{copy/insert metric} $m$ measures the upper
bound on the size of the optimal sequence of \emph{copy/insert}
instructions, described next.  Assuming an 8-bit byte, this metric
estimates the total delta size in bits:

\begin{align}
m (C^d, I^d) &= \sum_{i = 1}^{z^d} m(c^d_i) \\
m (\icpy{m}{o}{l}) &= 1 + \lceil\lg l\rceil + \lceil{\lg k^d}\rceil + \lceil{\lg o}\rceil \\
m (\iins{l})       &= 1 + \lceil\lg l\rceil + 8 l
\end{align}

Earlier work on copy/insert algorithms has used a simpler metric,
treating all copies with unit cost.  It then suffices to show that a
simple greedy algorithm is optimal \cite{Burns:Thesis}.  The unit-cost
copy metric is not an accurate estimate of the file difference, so I
propose the above.  The greedy solution is not optimal for the refined
metric.

\subsection{The Greedy Algorithm for Copy/Insert Deltas} \label{sec:greedy}

The greedy algorithm simply scans through the \emph{To} file, finding
the longest match at each position and generating a copy instruction.
All unmatched strings produce an insert instruction.

\begin{alg}\label{alg:generate}
Compute $(C^d, I^d) = t^d \ominus F^d$ where $d$ has a copy/insert
encoding
\end{alg}

Assume that match $(m, o, l) = \lmat (F,t,i)$, where the longest match
to $t$ starting at index $i$ in $F$ is $f_m[o] \ldots f_m[o+l-1]$.
Another parameter, $s$, is used as a lower bound on the length of a
copy instruction.  For now, assume $s=0$.  Given these, a program to
compute greedy copy/insert deltas is given in Figure
\ref{fig:generate}.

\begin{figure*}
\begin{program}
ge\= ne\= ra\= te\= De\= lta($t^d$, $F^d$) \\
  \> $i \leftarrow 0$; $z \leftarrow 0$; $I^d \leftarrow$ \textrm{\emph{empty}} \>\>\>\> \hspace{6cm} \= $\triangleright$ \hbox{Step 1: Initialize.} \\
  \> while ($i < \size(t)$) \>\>\>\>\> $\triangleright$ \hbox{Step 2: Loop.} \\
  \>\> $(m,o,l) \leftarrow \lmat (F^d,t^t,i)$ \\
  \>\> if ($l \le s$) \\
  \>\>\> if ($z > 0$ \textrm{\emph{and}} $c_z^d = \iins{j}$) \>\>\> $\triangleright$ \hbox{Step 3: Insert.} \\
  \>\>\>\> $c_z^d \leftarrow \iins{j+1}$ \\
  \>\>\> else \\
  \>\>\>\> $z \leftarrow z + 1$; $c_z^d \leftarrow \iins{1}$ \\
  \>\>\> $I^d \leftarrow I^d$ \textrm{\emph{append}} $t^d[i]$; $i \leftarrow i + 1$ \\
  \>\> else \\
  \>\>\> $z \leftarrow z + 1$; $c_z^d \leftarrow \icpy{m}{o}{l}$; $i \leftarrow i + 1$  \>\>\> $\triangleright$ \hbox{Step 4: Copy.} \\
  \> $z^d \leftarrow z$ \>\>\>\>\> $\triangleright$ \hbox{Step 5: Finish.}
\end{program}
\caption{Algorithm \ref{alg:generate}}\label{fig:generate}
\end{figure*}

Steps 1 and 5 execute once in $O(1)$ steps.  In the worst case, the
loop beginning in step 2 is executed $\size(t)$ times.  Each execution
of the loop requires $O(1)$ steps, plus the time taken by $\lmat$.
Altogether, he algorithm runs in $O(\size(t) \cdot M)$ steps, where
$M$ is the time complexity of the $\lmat$ operation.

Burns \cite{Burns:Thesis} shows that this algorithm is optimal when
copies are treated with unit cost.  Burns and also Tichy
\cite{Tichy84} demonstrate various $\lmat$ functions that all require
linear time and space.  An algorithm requiring only constant space is
required to operate on arbitrarily large files.

None of the available constant-space improvements, however, apply for
multiple input files, which will be used for file archiving.  As a
result, \xd {} uses a linear-space approximation of the greedy
algorithm.

\subsection{The \xd {} Delta Algorithm}

\xd {} uses a fast, linear time and space approximation to the greedy
algorithm.  The actual space required is chosen to be a fraction of
the input size less than approximately 0.5.  A \emph{fingerprint}
function is a hashing function for fixed-length strings for which
collisions are unlikely.  The algorithm uses a hashed index of the
value of a fingerprint function at regular offsets in the \emph{From}
data to perform string matching.  It then scans the \emph{To} file
performing the greedy algorithm.  The approximation arises from hash
and fingerprint collisions, and the approximate nature of the
string-matching function.

For some constant $s$ chosen to be a small power of 2, the algorithm
computes a fingerprint on segments of length $s$ in each file $f$ at
all offsets $i$ divisible by $s$ except, possibly, the last if $s$
does not divide $\size(f)$---in this case the segment is too short.
The fingerprint function $a_{fi} = \fingerprint(f, i)$ is evaluated in
each file and inserted into a hash table, $B$, with hash function $H$.
The number of such records is denoted $\records(F) = \sum_{f \in F}
\lfloor \frac{\size(f)}{s} \rfloor$.  The hash table does not chain
hash collisions.  Only a single $(f,i)$ pair or $\nil$ is recorded for
each bucket.  The hash table is implemented as an array of 32-bit
words of length $\records(F)$.  The $\nil$ value is encoded as the
value 0, and $(f,i)$ is mapped into the remaining values.  Since $i$
is bounded by $\lfloor \frac{\size(f)}{s} \rfloor$, $(f,i)$ can be
easily fit into a 32-bit word provided that the number of files in $F$
is less than $s$ and the length of $f$ is less than $2^{32}$.  Another
array of words $c_{f}$ is used for each file to index $c_{f}[i] =
a_{fi}$ so that a fingerprint collision can be easily detected.  The
sum of lengths for these arrays is also $\records(F)$, so in bytes the
algorithm requires:

\begin{equation}
O\left(\sum_{f \in F} 8 \lfloor \frac{\size(f)}{s}  \rfloor\right)
\end{equation}

\noindent for the two arrays, where 8 is the sum of 4 bytes per 32-bit
word for each of the separate data structures.  The choice of $s$ will
be discussed later, but should be chosen to be at least $2^4$, so the
space required to run the algorithm is bounded by:

\begin{equation}
O\left(\frac{8 \cdot \size(F)}{s}\right) \le O\left(\frac{\size(F)}{2}\right) = O(\size(F)) \label{action-bound}
\end{equation}

Algorithm \ref{alg:match} locates matching strings at each offset in
the \emph{To} file, supplying the match function used in algorithm
\ref{alg:generate}.

\begin{alg}\label{alg:match}
Compute $(m,o,l) = \lmat (F,t,i)$.
\end{alg}

\begin{figure*}
\begin{program}
ma\=tc\=h($t^d$, $F^d$) \\
  \> $x \leftarrow \fingerprint(t,i)$ \> \hspace{9cm} \= $\triangleright$ \hbox{Step 1: Fingerprint. } \\
  \> if ($B[H(x)] = \nil$) \>\> $\triangleright$ \hbox{Step 2: Lookup.} \\
  \>\> \textrm{\emph{return no match}} \\
  \> else \\
  \>\> $(m,o) \leftarrow B[H(x)]$ \\
  \> if ($c_{f_m}[o] \not = x$) \textrm{\emph{return no match}} \>\> $\triangleright$ \hbox{Step 3: Test for collision.} \\
  \> $l \leftarrow$ \textrm{\emph{length of longest matching substring at offsets $o$ and $i$}} \>\> $\triangleright$ \hbox{Step 4: Grow.} \\
  \> if ($l < s$) \textrm{\emph{return no} match} \\
  \> \textrm{\emph{return}} $(m,o,l)$ \\
\end{program}
\caption{Algorithm \ref{alg:match}}\label{fig:match}
\end{figure*}

The fingerprint can be efficiently computed incrementally while
advancing through the \emph{To} file.  The two characters entering and
leaving the fingerprint range are used to advance the result in $O(1)$
steps.  Locating string matches at each index takes, in the worst
case, $O(s) = O(1)$ steps for a failed match and $O(\size(t))$ for a
successful one, but the total number of steps expended during all
successful matches is bounded by $O(\size(t))$.  Using the result of
\S\ref{sec:greedy}, this gives a total time complexity of
$O(\size(t))$ for scanning the \emph{To} input, plus $O(\size(F))$ for
constructing the index.

\xd {} uses the \texttt{adler32} fingerprint function originally used
in gzip \cite{Rsync}.

\subsection{Analysis}

A large value of $s$ reduces the work done and space used by the
algorithm, but will locate fewer matches.  Too small a value of $s$
can lead to copies so small that they they are less efficient than an
equivalent insert encoding.  This last consideration gives a lower
bound on $s$:

\begin{align}
m (\iins{s}) &> m (\icpy{f}{o}{s}) \\
8 s          &> \lceil{\lg k^d}\rceil + \lceil{\lg o}\rceil \label{eqn:s}
\end{align}

\noindent The right-hand-side of (\ref{eqn:s}) is bounded by $\lceil
\lg \size(F) \rceil$, yielding $s > \frac{1}{8}\lceil \lg \size(F)
\rceil$.

Assuming the inputs, hash values, and fingerprint values have uniform
distributions, the approximations made so far can be expected to
perform well, as I will show.  The key insight is that if a long match
fails at one position due to a hashing collision, then it is likely to
succeed at the next matching offsets.  That is, a failure to match
$t[i \ldots i+s-1]$ at $f_m[sk \ldots s(k+1)-1]$ because $(m,k)$ is
collides in the hash table is not a disaster because the next matching
pair $t[i+s \ldots i+2s-1]$ and $f_m[s(k+1) \ldots s(k+2)-1]$ may
succeed.  The expected number of misses for a long match is
proportional to the expected number of hash collisions, and therefore,
the match is expected to fail asymptotically $O(1)$ times before the
match is discovered.  This causes an expected degradation that is
linear only in the number of instructions.

The hash function and fingerprint function, composed, reduce segments
in $F$ to integers less than $\records(F)$, which is chosen as the
table size.  When a conflict occurs during hashing, the algorithm must
choose a winner.  Several policies could be chosen.  For example, a
match appearing first in a file is potentially more valuable to the
matching procedure since it may locate a match that contains
colliding matches that occur later in the same file.  Here, I will
analyze a uniform random trial to decide whether to replace the
incumbent record or not.  By selecting the winner at random, the
distribution of the hashing functions discussed above is not altered.
Contrary to this analysis, however, simpler policies that never or
always replace the colliding record seem to work equally well.

Another array $trials[i]$ is required to contain the number of
collisions which have occured in the $i^{th}$ bucket.  The incumbent
record or $\nil$ in bucket $j$ is replaced with probability
$P(\textrm{\emph{replace}}) = \frac{1}{trials[j]+1}$.  The additional
array increases the space required by the algorithm in
(\ref{action-bound}) by at most a factor of $3/2$, since it brings the
number of arrays from 2 to 3.  The probability $P_k(i)$ represents the
probability of the $k^{th}$ record surviving the $i^{th}$ trial.
Then,

\begin{equation}
P_k(n) = \frac{1}{k} \cdot \frac{k}{k+1} \cdots \frac{n-2}{n-1} \cdot \frac{n-1}{n} = \frac{1}{n}
\end{equation}

\noindent so the probability of survival is uniform.

The string matching function \emph{fails} when a record that would
have matched was discarded in a collision.  The probability of failure
is determined by assuming the hash functions behave randomly and
computing the expected number of hash collisions.  After hashing $M =
\records(F)$ records into $N$ buckets, the expected number of of empty
buckets is:

\begin{equation}
E(\emph{\# of empty buckets}) = N \left( 1 - \frac{1}{N} \right)
\end{equation}

\noindent By choosing $N=M$, the expected number of empty buckets, and
therefore collisions, is asymptotically $N \inv{e} \approx 0.63 N$.
The expected number of trials before a successful match is found is
the expected number of failures $E(failed) = \frac{e}{e-1}$.  For
files relatively large compared to $s$, the matching algorithm is
expected to succeed after $\frac{e}{e-1}$ failures.

In addition to losing matches from hash collisions, matches are lost
from only searching at a fraction of the offsets in $F$.  For matches
of length $l \ge s$, the match can be expected to be discovered at one
of the $\lfloor \frac{l}{s} \rfloor$ interior points recorded in $F$.
The first such point may be offset at most $s-1$ bytes from the true
beginning, and the $q^th$ such point is located at most $s q + s - 1$
bytes from the true beginning.  From the above, we expect $q =
\frac{e}{e-1}$, so the approximations produce matches which are
expected to shorter than their true length by $\frac{s e}{e-1} - s -
1$ bytes due to hash collisions.  Thus, the expected penalty is linear
in the number of copies, not the length of the inputs.

\subsection{Inversion of Copy/Insert Deltas} \label{sec:invert}

Given file $f$ and paired delta $d$ that computes $t$ from $f$, we
would like to compute a delta $\inv{d}$ that computes $f$ from $t$.
The solution will set up more general, direct delta manipulation in
the next section.  One might possess $f$ and $d$, and want to inform a
peer possessing $t$ how to construct $f$ by sending $\inv{d}$.

\begin{alg}\label{alg:invert}
Compute $\inv{d}$ from $d$ and $f$.
\end{alg}

I use a balanced interval tree to construct the reverse mapping for
copy instructions in $d$.  The tree supports insertion, deletion, and
location of elements in $O(\lg V)$ steps for $V$, the number of
intervals (see, for example, Cormen, Leiserson, and Rivest
\cite{clr}).  The program in Figure \ref{fig:invert} computes the
interval table $R$.

\begin{figure*}
\begin{program}
in\= ve\= rt\= Copies ($f$, $d$) \\
  \> $i \leftarrow 0$ \>\> \hspace{6cm} \= $\triangleright$ \hbox{Current instruction in $d$.} \\
  \> $o_t \leftarrow 0$ \>\>\>             $\triangleright$ \hbox{Current offset in $t$.} \\
  \> $R \leftarrow \textrm{\emph{empth}}$ \>\>\> $\triangleright$ \hbox{Empty interval tree.} \\
  \> while ($i < z^d$) \\
  \>\> if ($c_i^d = \icpy{f}{o}{l}$) \\
  \>\>\> $R([o_t \ldots o_t + l)) \leftarrow [o \ldots o + l)$ \\
  \>\>\> $o_t \leftarrow o_t + l$; $i \leftarrow i + 1$ \\
  \>\> else ($c_i^d = \iins{l}$) \\
  \>\>\> $o_t \leftarrow o_t + l$; $i \leftarrow i + 1$ \\
\end{program}
\caption{Invert Copies in $d$ from $f$}\label{fig:invert}
\end{figure*}

Using \texttt{invertCopies} to construct $R$, $\inv{d}$ is constructed
with an in-order traversal of $R$.  Each range $r$ records a mapping
from $[o_f \ldots o_f + l)$ in $f$ to $[o_t \ldots o_t + l)$ in $t$.
Each such range may be used to construct $\icpy{t}{o_t}{l}$.  Insert
instructions are generated for all ranges not covered by $R$.

The performance of this algorithm depends on $z^d$, $z^{\inv{d}}$, and
the $\size(I^{\inv{d}})$.  First, however, $z^d$ and $z^{\inv{d}}$ can
be related.

\begin{thrm} \label{thrm:zrel}
$z^{\inv{d}}$ is bounded according to:
\begin{equation}
z^{\inv{d}} \le 2 z^d + 1
\end{equation}
\end{thrm}

{\sc Proof} Let $z^{\inv{d}}_i$ by the number of instructions in
$\inv{d}$ as a function of the number of instructions $i$ processed so
far in $d$.  Observe that $z^{\inv{d}}_0 = 1$, corresponding to a
single insert instruction in $\inv{d}$.  In the worst case, each
copied range may remove one existing instruction and create three; the
induction hypothesis is $z^{\inv{d}}_{i+1} = 2 + z^{\inv{d}}_{i}$. \qed

The algorithm given does not produce an optimal delta, however.  Its
performance degrades when $t$'s copied segments overlap in $f$--the
previous analysis also suffers from repeated segments of length
greater than $s$.  Neglecting this case, the size of the delta is
$\size(\inv{d}) \approx \size(f) - (\size(t) - \size(d))$.

The \texttt{invertCopies} procedure executes a loop $z^d$ times, each
requiring $O(\lg z^d)$ steps.  The final construction, given $R$,
requires $O(z^{\inv{d}} + \size(I^{\inv{d}}))$ steps to compute the
instructions in $\inv{d}$.  Together, and using the results of theorem
\ref{thrm:zrel}, the algorithm requires $O(z^d \lg z^d +
\size(I^{\inv{d}}))$.  This time bound is acceptable, since $z^d$ is
expected to be small--it corresponds to the number of non-overlapping,
longest matching substrings between $t$ and $f$.

\subsection{Applying Copy/Insert Deltas}

A simple implementation of \emph{apply} for copy/insert deltas is
straightforward and requires $O(\size(t))$ steps to apply $d$.  The
\emph{reconstruct} operation takes a number of deltas and applies them
all at once.  Reconstruct accepts a tree of deltas in which each leaf
is represented by a literal file and each internal node is represented
by a delta, and constructs the file that the root node's delta
constructs.  It can be easily extended to operate on DAGs.
Reconstruct can be considerably more efficient than simply applying
each delta in sequence.

The technique for reconstruction resembles the inversion algorithm
above.  Suppose that $t_r$ is being reconstructed from the delta tree
rooted at $d_r$, where interior delta nodes are denoted $d_i$ and
leaf, literal nodes are denoted $l_i$.  Algorithm
\ref{alg:reconstruct} computes an interval tree mapping ranges in
$t_r$ to the literal data it is ultimately constructed from: files
$l_i$ and insert data $I^{d_i}$.  It computes in a bottom-up traversal
of the tree.  A range $[l \ldots h)_{f,o_f}$ indicates the mapping
from $[l \ldots h)$ to $[o_f \ldots o_f + h - l)$ in $f$, where $f$
can also be the insert-data of some delta $I^d_i$, which is treated as
just another literal file.  Finally, the computed interval tree for
$d_r$ is used to write out the result in $O(\size(t_r))$ steps.

\begin{alg}\label{alg:reconstruct}
Reconstruct $t_r$ from $d_r$.
\end{alg}

The program for computing $t_r$ is given in Figure
\ref{fig:reconstruct}.  The function \texttt{translate} translates the
interval $[o \ldots o + l)$ from interval tree $R_f$ to interval tree
$R_t$.  Each call to \texttt{reconstructRange} requires, in addition
to the recursive calls, $O(z \lg z)$ steps and $O(z)$ space to insert
$z$ ranges into the resulting interval tree.  For literal files, the
call executes in constant time.  For each delta $d$, $z$ is bounded by
$z^d$ times the maximum of the number of ranges in each child's
interval map.

The maximum number of ranges for delta $d_1$ due to the path $d_1
\rightarrow \ldots d_n \rightarrow l_i$ is $\prod_{i=1}^{n}{z^i}$, but
is bounded by $O(\size(t^{d_1}))$ since there are only so many
intervals.  The worst case occurs when a copy instruction copies from
a range being constructed from the child delta which includes the
range of every copy instruction in the child, and is unlikely in
practice.

Let value $z^*$ be the maximum such product for $d_r$.  The
intermediate storage required is the size of the largest interval
table, which is $O(z^*)$.  The time to reconstruct $t_r$ from a tree
of $n$ deltas is $O(n z^* \lg z^* + \size(t_r))$.  This time
complexity will be further examined for the archival scheme based upon
it which follows.

\begin{figure*}
\begin{program}
re\= construct($d_r$) \\
  \> $R$ = reconstructRange($d_r$) \\
  \> fo\= r (increasing ranges $r = [r_l \ldots r_h)_{l_r}$ in $R$) \\
  \>\> output the segment beginning at $r_l$ in $l_r$ \\
re\= co\= ns\= tructRange($d_i$) \hspace{3cm} \= $\triangleright$ \hbox{for a delta} \\
\> $R =$ \textrm{\emph{empty interval tree}} \\
\> $o_i = 0$ \>\>\> $\triangleright$ \hbox{offset in $I^{d_i}$} \\
\> $o_t = 0$ \>\>\> $\triangleright$ \hbox{offset in $t^{d_i}$} \\
\> for ($f \in F^{d_i}$) \\
\>\> $\intervals[f] =$ reconstructRange($f$) \\
\> for (each instruction $c_i$) \\
\>\> if ($c_i = \icpy{f}{o}{l}$) \\
\>\>\> translate ($R, intervals[f], o, l$) \\
\>\>\> $o_t = o_t + l$ \\
\>\> else if ($c_i = \iins{l}$) \\
\>\>\> insert ($R$, $[o_t \ldots o_t + l)_{I^{d_i},o_i}$) \\
\>\>\> $o_i = o_i + l$ \\
\>\>\> $o_t = o_t + l$ \\
\> for ($f \in F^{d_i}$) \\
\>\> free $\intervals[f]$ \>\> $\triangleright$ \hbox{no longer required} \\
re\= co\> ns\> tructRange($l_i$)              \> $\triangleright$ \hbox{for a literal file} \\
\> $R$ = \textrm{\emph{empty interval tree}} \\
\> insert ($R, [0 \ldots \size(l_i))_{l_i,0}$) \\
\> return $R$ \\
\end{program}
\caption{Reconstruct}\label{fig:reconstruct}
\end{figure*}

Observe that this procedure allows a version to be partially
extracted.  The time complexity to extract a partial version of length
$p$ is still governed by $\size(t)$ from the $z^* \lg z^*$ term, but
only in the worst cast.  Otherwise, the additional $\size(t)$ term is
reduced to $\size(p)$, which is potentially significant.

\subsection{Experimental Results}

Several programs for computing file deltas are compared to evaluate
their performance relative to the copy/insert metric $m$ defined in
\S\ref{sec:metrics}.  A total of 25584 file pairs that changed
in the Linux kernel releases between versions 2.1.0 and 2.1.28 were
used; both forward and reverse deltas were computed and the ratio of a
program's delta size to the metric is plotted against the \emph{To}
file size in Figure \ref{sizes}.  The results are averaged on the
graph, with error bars indicating the standard deviation.

For the purposes of this abstract, only the results \xd {} and
\emph{vdelta} are shown.

\medfigure{sizes}{Relative delta sizes.}

For small files, \emph{vdelta} outperforms \xd, but as the file size
increases its performance degrades.  In fact, \xd's relative
performance continues to improve as input size grows, but the
copy/insert metric was too expensive to compute for larger files so
these results could not be included in the same plot.  The performance
of \xd {}  and \emph{vdelta} are, therefore, directly compared in Figure
\ref{bigsizes} for files from 25 to 30 megabytes in size.  These
files were the tarred contents of each of the Linux kernel releases
used above.  This test is especially relevant for determining how
practical it is to release software package changes as a delta against
the previous release, not on a file-by-file basis, but from the old
package to the new package.

\medfigure{bigsizes}{Delta sizes for large files.}

The linear-space approach has paid off on large input files.
\emph{Vdelta}'s delta size is close to the compressed size of the
\emph{To} file for many of the files in Figure \ref{bigsizes}, little
space has been saved over simply compressing the \emph{To} file.  This
is likely to be a result of the constant-space assumptions failing,
and because the contents of the files may have been reordered.

\emph{Unfinished}.

\subsection{Future Work}

Improvements in the space required by multi-input delta functions are
necessary future work.

The reconstruct algorithm given has a similarity to the nearly linear
time scheme presented for an editor with built-in version control by
Fraser and Myers \cite{Myers:Editor}, though it does not directly
apply.

The results achieved by the \emph{vdelta} algorithm are very
promising, but do not directly extend to all of the techniques
presented here.  Along these lines, the techniques presented below
suggest that considering the delta problem in isolation is not the
best way to proceed.  There are strong connections to data compression
and text retrieval, here; these can be taken advantage of when
applying delta algorithms to particular application domains.

\section{Versioned-File Archives}\label{sec:archival}

A versioned file archive supports the \emph{get}, \emph{put}, and
\emph{delete} operations.  \emph{Put} inserts a file into the archive,
returning a name by which to retrieve it, and \emph{get} retrieves the
file.  \emph{Delete} indicates that a file will never be required
again and that it's storage may be reclaimed.

The archive stores multiple versions of a \emph{family} of files that
are expected to be similar.  Typically, an ancestry tree is imposed
upon the archive; every version except the root has a parent.  Changes
between parent and child are stored as a delta.  Requiring an
application to know the ancestry proves difficult and inefficient, as
this information is not always available and the storage model can
lead to unpredictable performance.  This mechanism has succeeded
because the version control utilities which use it trusts users make
the right decision by understanding the model, version naming
conventions, and performance implications.  Implementing efficient
storage management on top of RCS, for example, proves difficult for
these reasons \cite{MacDonald:1998:SCM}.

The goal is to reduce the space required to store many versions in a
family by taking advantage of their similarity and possibly their
ancestry, while maintaining high performance and reliability for the
\emph{get}, \emph{put}, and \emph{delete} operations.

\subsection{Related Work}

Compact, versioned-file archiving can be achieved by storing literal
files along with delta chains or trees, but other solutions are
possible.  Techniques for storing multiple versions have been studied
and applied in several application areas: database management systems,
programming language environments, version-control, and file-backup.

Both SCCS and RCS store an ancestry tree of versions where each vertex
represents a version and each edge represents a delta.  One version is
stored as a literal file $L$ and the rest are reconstructed by the
application of deltas to the literal file.  An archive using this
organization often has unpredictable performance because the
efficiency of an operation often depends on its distance from $L$.

SCCS doesn't actually store a literal file with separate deltas, but
merges them together.  Any version can be extracted in a single pass
through the archive, collecting only those lines that are present in
the version being extracted.  This requires time linear in the size of
the archive, including all deltas, not the file being extracted.

RCS improves upon this model by storing the literal file and deltas
separately.  The literal file is chosen to be a particular leaf node,
causing retrieval and insertion on other branches to be much more
expensive than on the distinguished \emph{trunk}, which contains the
literal file.  Accessing any other branch requires the application of
a series of deltas---the reconstruct problem.  RCS builds a data
structure representing a series of lines and applies each delta in
succession.  Let $L$ be the sum of line counts for the deltas being
applied.  Reconstruct requires at least $O(L \lg L)$ steps using a
balanced tree, though RCS in fact uses an $O(L^2)$ algorithm for
simplicity\cite{Yu:1994:LTS}.

Rosenkrantz and Yu demonstrate a reconstruct algorithm requiring
$O(L)$ steps by computing a special form of the insert/delete delta
\cite{Yu:1994:LTS}.

Severance and Lohman show how to use file deltas for the storage of
versions of large objects in databases and, more generally, for
implementing transactions \cite{Severance:1976:TODS}.  Alderson
presents a space-efficient technique based on B-trees and file deltas
that allows partial records to be retrieved without fully constructing
the entire record, and for which the get operation has a predictable
time-complexity for all versions \cite{Alderson:1988:JSE}.  Fraser and
Myers demonstrate an editor with built-in version-control capabilities
that uses AVL dags.  It too has a nearly linear-time reconstruct
operation \cite{Myers:Editor}.  These other storage schemes apply
equally well to the file-archiving problem as presented, but
constructing an archive out of deltas allows for the efficient
transfer of archive components.

Burns and Long describe a technique for distributed backup based on
delta compression \cite{Burns:Backup}.  Instead of a version tree,
however, they use a technique called \emph{version jumping} which
involves storing a number of \emph{clusters} consisting of one
literal file and unit-length delta chains.  This allows them to
guarantee a file can be reconstructed with only two file accesses,
which is important when slow storage is used.  Analysis is performed
to decide when a new literal file should be created, rather than a
delta.

%\subsection{Problems With RCS}

%PRCS uses RCS to implement efficient storage for a higher-level
%version control system.  During its development, many problems with
%RCS were encountered.

%The \emph{version tree} storage model is flawed for two reasons:

%\begin{itemize}
%\item It discriminates against versions which are not $L$'s direct
%ancestors.  The cost of inserting or extracting a file depends upon
%its ancestry, not its age.  This can often be integrated into the
%application so that the user is aware of the penalties, as with RCS,
%but just as often not.
%\item Higher level version control utilities which do not wish to
%present this implementation detail and performance penalty to the user
%must arbitrarily choose new versions, as it cannot guess what the user
%will do in the future from the previous insertion or retrieval
%operations.
%\end{itemize}

%\noindent The semantics of a version tree do not make for a good
%implementation of the required interface since the performance is
%unpredictable.  In addition to problems with the version tree storage
%model:

%\begin{itemize}
%\item The delta mechanism is line-oriented and therefore unacceptable
%\item Until the recently there was no complete library implementation
%of RCS.  Even so, the interface is cluttered by features implementing
%a particular version control policy
%\item Archive compression is expensive to achieve, since RCS does not
%directly support compression.  An archive could be compressed in such
%a way that only the necessary portions needed uncompression
%\item There is no guarantee that deleted space is reclaimed.  A
%version may remain necessary because it is on a path linking other,
%live versions.  Though it may be desirable to automatically discard
%versions as they reach a certain age, this restriction makes it
%difficult.
%\end{itemize}

%\noindent These considerations and more to come called for a new
%archival technique.

\subsection{File Archiving With \xd}

The \xd {} archive organization was originally designed as a
replacement for RCS to be used in a distributed version control
system.  It stores a DAG of both literal files and deltas.  Using an
extension to the techniques discussed in \S\ref{sec:delta}, it
also allows new versions to be efficiently transferred between
archives as deltas.  The internal organization is described here, and
network communication is described in the next section.

\xd {} does not depend on a file's ancestry for organizational
purposes.  The archive consists only of a series of versions in
increasing order of age.  As branches of the family tree diverge, even
though deltas between a version and its parent may be small, deltas
between two leaves of the family tree may be quite large.  Therefore,
storing deltas in a simple series threatens to be quite inefficient
since these \emph{branch-crossing} deltas between leaves may require
much more storage than the equivalent encoding using only deltas
between parent and child.  However, performance is critical, so
storing versions as a simple series is appealing because it makes the
time to retrieve a version to grow with its age, not as a function of
its ancestry.

This suggests the organization of the \xd {} archive--simply a series
\emph{clusters} as in the \emph{version-jumping} technique.  The
organization allows the time to retrieve any version to be bounded,
while still addressing the diverging branch issue, using a combination
of version-jumping and DAGs of deltas.  When the time to retrieve the
eldest version in a cluster reaches some threshold, its growth is
halted.  As with version-jumping, new literal files are occasionally
introduced into storage to maintain several invariants.

\bigfigure{dependency}{Sample \xd {} Cluster}

When inserting a version, either a \emph{forward} delta is inserted to
produce the new version from the most recent literal file in the
archive, or the new version becomes the literal file and a
\emph{reverse} delta replaces the previous literal file.  The decision
to choose one over the other will be addressed later.  In either case,
the contents of the insert-data of recent deltas are used as
additional sources for \emph{copy} instructions for the new delta, as
illustrated in Figure \ref{dependency}.

\bigfigure{onestep}{Sample Deltas}

Figure \ref{onestep} illustrates the process on eight string versions
$F1 \ldots F8$.  The one literal file is $F7$.  For demonstration, the
example uses a simpler delta algorithm that only copies similar
characters when they occur at the same offset in both files.  In a
realistic example, each character would also be represented by a
longer, matching segment.  The first column represents the full text
of each string; these strings were derived according to the ancestry
tree in Figure \ref{strings}--each parent-child relation contains at
least six matching characters.

\bigfigure{strings}{String Relations in Figure \ref{onestep}}

When the first version is inserted into the archive, it becomes the
literal file.  Subsequent versions produce reverse deltas for some
period of time until the time to extract the first file becomes
excessive, at which point the literal file is \emph{frozen}.  Forward
deltas may then be inserted until the storage efficiency degrades, as
measured by relative delta size, for example.

The time required to insert a version is always just the time to
compute a single delta.  Deletion requires $O(1)$ time.  Retrieval is
implemented with reconstruct, whose time complexity is $O(n z^* \lg
z^* + \size(t))$ to compute version $t$ from $n$ deltas, where $z^*$
is bounded by the maximum product of the instruction counts on all
paths from root to leaf and also by $\size(t)$.  Even assuming a small
number of changes in each version, the time to retrieve a version is
still $O(n \size(t))$, which is an unacceptable rate of growth, left
unchecked.  The maximum depth of a reverse delta DAG effects the time
required to retrieve the oldest version in a cluster, and therefore
determines when the literal file should be frozen.

Burns and Long show that version jumping does not result in a great
loss of storage efficiency \cite{Burns:Backup}, and derive a
worst-case analytic point at which storing a new literal file should
be introduced as a function of the \emph{compressibility}, or
fractional size, of the delta.  A simple heuristic for deciding to
introduce a new literal file is used: introduce a new literal file
when inserting a delta will increase the compression ratio of the
cluster measured as the total size of all versions divided by the
total size of all deltas and the literal file.  The cycle of reverse
deltas, followed by a literal file, followed by forward deltas,
followed by a version jump repeats indefinitely.

%The storage efficiency in \xd {} is improved over the version jumping
%scheme, and slightly different, since previous deltas' insert-data
%were not used in Burn's calculations--these only improve forward delta
%storage efficiency.

In Figure \ref{onestep}, the columns illustrate the relation between
each file and the delta that constitutes it.  In the control and
insert-data columns, a shaded region indicates that the text is either
copied or inserted at that position in the file.  In the control
column, two shadings are used to distinguish copies from the
subsequent literal file and copies from previous deltas.  For
instance, the characters \texttt{EN} in \texttt{ESSELENIAN} are copied
from the insert-data segment of the delta representing
\texttt{DISELENIDE}.  The \texttt{DIS} in \texttt{DISELENIDE} is
copied from the literal file \texttt{DISULPHONE} which follows it.
This illustrates how using previous deltas handles diverging files.
The intervening insertion of \texttt{DISULPHINE}, on a different
branch, does not disturb the encoding efficiency.  The number of
previous deltas used must be somehow chosen and limited in number.  A
window size $w$ is chosen according to available space.  Increasing
$w$ promises more available matches, but at the same time unrelated
data threatens to evict matching data from the hash table.  The
previous deltas chosen are the most recent, but other strategies for
locating possibly related files can be used.  The ancestry of a
version can be used, in addition, to select the most recent delta
computed \emph{to} an ancestor of the one being inserted, it is likely
related.

In addition to these properties,

\begin{itemize}
\item \xd {} uses an efficient, non-line-oriented delta mechanism
\item The archive can trade time for space by precomputing and
adjusting the delta DAG depth
\item The number of deltas that must be applied to retrieve a version
is proportional to the number of put operations since that version was
created; RCS does not have this property, for example
\item Knowledge of a versions ancestry is not required for
organizational purposes
\item Partial compression is possible--older versions may be
compressed without affecting the retrieval time of newer versions, and
when combined with precomputed indices, they may be lazily
uncompressed
\item Obsolete deltas may be simply deleted as soon as no other live
deltas reference their insert-data.
\end{itemize}

%\item Higher level tools are free from organizational concerns when a version's ancestry is unavailable
%\item The implementation was designed for use as a library, and includes no policy

\subsection{Experimental Results}

\emph{Unfinished}.

%%Ideally: Compare storage, put, and get measurements for my scheme
%%vs. version jumping vs. forward/reverse chaining vs. store each.  on
%%branch, off branch, other positions.  compressed or uncompressed.  On
%%two random, evolving files, then on the FreeBSD source dir.

%%Begin with an RCS file.
%%Run once compressed, once uncompressed.
%%Have an API for each checkin.  One client per policy.
%%The API computes the scores.

%An experiment was done comparing the RCS and {\sc Xdelta} archives for
%each of the RCS files extracted from FreeBSD's CVS repository in the
%\texttt{src/sys/vm} subdirectory, taken on May 3, 1998.  The
%collection totaled 35 files and 1608 versions, of which only 134
%versions were off the RCS trunk.  The average number of versions per
%file is 45.9, with a standard deviation of 39.6.  The resulting
%uncompressed archive sizes are as follows:

%\begin{center}
%\begin{tabular}{|l|l|} \hline
%Archive size & (bytes) \\ \hline
%RCS          & 1617850 \\ \hline
%{\sc Xdelta} $(w=10)$ & 1686808 \\ \hline
%\end{tabular}
%\end{center}

%Due to space limitations in the abstract, more complete results are
%omitted.  The results show that the new scheme is not much worse than
%RCS in terms of storage space, yet the \xd {} archive can exploit
%compression, where RCS cannot.

%The results show that for an uncompressed archive, the performance of
%{\sc Xdelta} is not much worse than RCS.  However, {\sc Xdelta} is
%capable of efficiently operating on compressed archives and RCS is
%not, so the advantages seem to outweigh the disadvantages.  The
%difference in compressed archive sizes is so great because the
%compression of many small files is not likely to be as good as the
%single compression of one large file.

\subsection{Future Work}

Delta compression techniques have proven successful, but archival
techniques have received little exploration.  As computers become more
and more dependent upon network communication and distributed data,
distributed file-archival techniques will increase in importance.

A popular method for viewing the history of a version-controlled
document involves attributing each line with the name of the author
and how recently it was changed or introduced.  Directly constructing
such an annotation from copy/insert deltas is not as
straight-forward, and more difficult to present since it is not
line-oriented.  Another type of annotation allows a file to be viewed
in a particular context, allowing not only how each line was authored,
but the author of the next modification or deletion.  More advanced
tools for viewing (or querying) relations between files in a
file-archive are needed.

The windowing technique technique presented for determining a set of
input files could be replaced with a coarse text retrieval algorithm
to determine the set of files likely to contain substrings of the
output file.

\section{Distribution Protocols}\label{sec:distribution}

There is a broad class of applications that operate on large
quantities of data, often highly similar or version controlled, and
need to move that data across slow networks frequently.  Delta
compression can be used both for versioned file archival, for reducing
network transmission, as a network data cache.  The effort required to
implement these operations efficiently, however, is considerable, and
has hindered advances so far.  In this section, I propose a simple,
yet powerful, abstraction for the purposes of constructing application
protocols, according to the following considerations:

\begin{itemize}
\item Data should be named by content.  The MD5 checksum
\cite{RFC1321}, for example, can be used to name a file; doing so
removes the need for any global consistency.
\item Versions are read-only and should be highly available; therefore
replication is required
\item Disk space is more available than network bandwidth
\item A moderate amount of computation for each client connection is
permitted, but should be minimized
\item Compressing data on a per-connection, per-transfer basis heavily
loads a server, and is undesirable, as evidenced by the maintainers of
popular CVS servers \cite{mozillaCvs}
\item Each server should be autonomous, so there are no global
consistency issues; deltas must be receivable out of order
\item Servers must be allowed to store disjoint sets of files
\item Servers should transfer files using a minimum of network
bandwidth
\item Pairwise peer synchronization should be used to exchange archive
contents; at the application level, synchronization can be pushed or
pulled, since the protocol is symmetric
\item A server should be easily usable as a proxy-cache.
\end{itemize}

\noindent A scheme that allows the direct exchange of pre-compressed
deltas is desirable.  Each archive maintains a set of versions for
each family in a hierarchically named set of file families.  The
application is responsible for deciding which files should be stored
at each server.  The underlying storage mechanism only provides
efficient delta-storage and push/pull delta-transfer.  Aside from
efficiency issues, this type of service resembles the operation of
NNTP servers for news transport as described in RFC 1036
\cite{RFC1036}.  This is a nice abstraction for building application
servers; an application may co-reside with a distribution server,
leaving the details of efficient transfer to the abstraction and only
implementing application specific control and policy.  Since the
service can be easily proxied, it reside on a client machine or a slow
network boundary.  This allows all the details of efficient transport
to be left to a decentralized, and thus locally administered, service,
so client to server protocols need not be effected.

All of this requires a small modification to the copy/insert delta
format, described next.

\subsection{Normalized Copy/Insert Deltas}

According to the above list of considerations, each server may store a
different set of files, and have received them in a different order
from everyone else.  Deltas depending on the insert-data of other
deltas, however, cannot be easily exchanged because a server's peer is
not guaranteed to have the same set of deltas, or to have received its
versions in the same order.  A \emph{normalized} copy/insert delta
includes additional information to avoid this problem.  The header for
delta $d$ names each dependent file $f^d_i \in F^d$ by its MD5
checksum.  Additionally, when $f^d_i$ is the insert-data of delta
$d_i$, it also includes the MD5 checksum of the file constructed by
$d_i$, $t^{d_i}$.  An alternate sequence of copy instructions is
included for each copy instruction in $d_i$, allowing the copy to be
performed either from the insert-data of the delta or the literal
file, whichever is available.  This allows, in many cases, a server to
send its peer a version by simply sending its existing delta, perhaps
pre-compressed.

These modifications increases the cost of a copy instruction by a
small constant factor, except in pathological cases where one version
contains literal copies of another delta in the archive.  A single
copy instruction may be followed by one or more normalized copy
instructions because copies from the insert-data of a delta may be
fragmented in the file it constructs.

\subsection{Related Work}

Douglis and others have studied the addition of delta compression to
\texttt{http} \cite{SIGCOMM97*181}.  Their results are encouraging,
showing substantial decreases in both end-to-end latency and network
transmission for several delta transfer strategies.  There is an
official W3 Consortium proposal for the distribution and replication
of \texttt{http} \cite{w3}.  It includes the specification of a
standard copy/insert delta format for exchange \cite{gdiff}.  The
protocol includes extension for replicating content versions using
delta transfers.  Content is also named by a configurable message
digest, such as MD5.  This work has not dealt directly with
implementation issues, but has specified an exchange protocol.

The \emph{rsync} algorithm, due to Tridgell and Mackerras, is for
remotely synchronizing the contents of a pair of files \cite{Rsync}.
It does this using a computation similar to algorithm
\ref{alg:generate}, but transfers a table of checksums for one file to
the peer who performs string matching in the other file using MD5
checksums rather than direct string comparison.  The peer then replies
with a request for the list of blocks that it is missing.  This is
quite effective for synchronizing files between two peers that do not
share any versions in common.

\emph{CVSup} \cite{Polstra96} is a distribution protocol for CVS that
globally synchronizes the contents of client CVS repositories with a
mirror or the master repository.  This is accomplished primarily by
decomposing RCS files in the repository and transferring only the
deltas that are missing on the client side.  This model requires
centralized control.

\subsection{A Distributed Archive}

A distributed archive consists of a network of autonomous archives as
described above.  An application is responsible for controlling each
archive appropriately.  I'll analyze a simple arrangement first, as it
demonstrates an important special case.  Once the simple arrangement
has been described, I will extend the results for less constrained
circumstances.

Suppose that for a file family being archived, the policy of the
application running on client $S_c$ is to replicate every version in
server $S_s$.  Then $S_s$ can efficiently send $S_c$ new versions as a
delta with little computation.  Only the literal file, for which no
delta is immediately available, requires processing, and the result
can be cached.  If the version being sent is already available as a
delta, it can just be sent.  If the distributed storage can be
arranged as a tree in which each child replicates at least its
parent's set of versions, this situation applies.

All of this relies on the normalized delta--it allows copies from both
a \emph{site-specific}, readily available segment and a
\emph{universal}, but perhaps not physically present, segment.  When
transferring a delta the site-specific segment may be missing at the
receiver.  All the sender need do is verify that the receiver can
construct each of the necessary universal segments, allowing it to
internalize the delta by translating it to depend upon its own
site-specific segments.  The client, therefore, is responsible for the
majority of the work required to receive.  Suppose that before
synchronization $S_c$ contains the set of versions $F_c$, and $S_s$
contains the set of versions $F_s$.  $S_c$ requests a transfer of the
difference $F_t = F_s - F_c$.

\begin{clm}
For each version $f$ to be sent, if $S_s$ is storing $f$ as delta
$d_f$ it can simply send $d_f$.  Otherwise $f$ is stored as a literal
file, and $S_s$ can compute and send a forward delta $f \ominus f_c$,
for some recent $f_c \in F_c$.
\end{clm}

{\sc Proof} Each delta that was sent must be applicable by the
receiver after the complete transmission.  Since the client requested
all versions, those that were stored and sent as deltas depend on
either a version that the client already has, one of the other deltas
that was sent, or a literal file for which a forward delta was sent.
Since the forward deltas that were computed depend only on versions
already in $F_c$, no cyclic dependencies are introduced at the
client. \qed

Assuming archives synchronize with each other with period $p$ and
versions are deleted when their age surpasses $a$, then an archive may
pre-compute only a single, forward delta for each literal file in the
archive.  The \emph{From} input for this computation should be the
first version whose age is greater than $p$ and less than $a-p$.  Each
server can also keep a record of its peers' contents to allow
incremental log updates and each archive to compute a forward delta
that the greatest number of its peers will be capable of applying.

\subsection{Arbitrary Policies}

The previous section dealt only with the case where the client's
policy is to archive every file archived by the server.  This special
case allows many new deltas to be sent uninterpreted because the
client replicates every version in the server.  The case of a literal
file is no different, but when a server must send an existing delta to
a peer replicating fewer versions than itself, it must translate out
the peer's missing references before sending.  Suppose that $S_c$
requests $F_t$, where $F_t \cup F_c$ is a subset of the server's set
of files $F_s$ and therefore $S_s$ may not be able to directly send a
delta.

Computing such a delta is essentially similar to the reconstruct
operation.  First, \texttt{reconstructRange} is used to construct an
interval tree $R$.  Each copied range in $R$ is used to either
construct a copy instruction, if $S_s$ has the necessary reference, or
is translated into an insert instruction if $S_s$ does not have the
reference.  This procedure reduces to the above special case when the
client has requested every file that the server contains.  Again, the
time complexity does not directly depend on the size of the version
involved, but on the number of changes and deltas between them.

\subsection{Receiving Normalized Deltas}

It is guaranteed that when an archive receives a delta from a peer, it
can apply that delta, though not as efficiently as possible.  In fact,
\texttt{reconstruct} will operate as given by simply using the
normalized copy indices when a segment is missing.  However, the
reconstruct algorithm given assumed that all cross-edges in the DAG's
depth-first-subtree were directed at literal files, allowing the
recursive algorithm to free a node's interval table immediately after
its parent has used it.  Foreign deltas do not maintain this
invariant; so an efficient implementation of \texttt{reconstructRange}
must instead compute interval trees in a bottom-up DAG traversal; each
node's interval tree can be discarded after each parent's
interval-tree has been computed.  This increases the space required by
the algorithm by, at most, a factor of 2, since one additional
interval-tree may required.

Given an implementation of \texttt{reconstructRange}, a delta can be
internalized by computing the range table mapping copy ranges to local
segment ranges, and re-generated as in algorithm \ref{alg:invert}.

\subsection{Future Work}

I have implemented the system described as the basis of a distributed
version-control system.  The synchronize operation is extended to use
the \emph{rsync} algorithm when peers share no files in common, and to
send the literal file when one peer contains no files.  Future work
will include a complete experimental analysis of the system's
performance and behavior.  Making such an archive operate securely and
with minimum trust requirement is a future challenge.

\section{Conclusion}

The file delta problem and several applications have been examined.
Many applications desires both compression, differential compression,
and other features which complicate the direct application of
available delta algorithms.  These additional considerations
influenced the design of a new system with a new delta algorithm.

The \xd {} delta algorithm requires linear space, but requires less space
than the size of the inputs, meaning the algorithm is feasible for
files which are required to reside in memory by other applications.
The algorithm trades space to improve delta size, and approximates the
exact algorithm to reduce execution space and time.  It handles
multiple inputs, a subject which has not previously been studied, and
performs especially well on large inputs.

The \xd {} archiving scheme was designed as a replacement for RCS with
network distribution in mind.  It replaces the ancestry tree with a
serial model using a combination of version-jumping, forward deltas,
and reverse deltas.  Multiple \emph{From} inputs are used when
computing deltas, allowing the contents of more than one previous
version to be referenced by copy instructions.

Finally, a scheme for decentralized network file archiving was
presented.  The copy/insert delta encoding is altered so that each
copy instruction is replaced by two sets of instructions, allowing a
two machines to directly exchange deltas without any computation in
some cases.  I propose the distributed file archive as a abstraction
for efficiently implementing certain types of distributed
applications, such as version-control, software-update, groupware, and
\texttt{http} caching and replication.

\bibliography{dcc99}

\end{document}
